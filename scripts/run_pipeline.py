"""
Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ Ïä§ÌÅ¨Î¶ΩÌä∏ (Purged Walk-Forward CV)

Ìïú Î≤àÏùò Ïã§ÌñâÏúºÎ°ú:
1. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
2. Î™®Îç∏ ÌïôÏäµ (Purged Walk-Forward CV)
3. OOF ÌèâÍ∞Ä
4. ÏµúÏ¢Ö ÌÖåÏä§Ìä∏ (ÎßàÏßÄÎßâ 180Ïùº)
5. Kaggle Ìå®ÌÇ§Ïßï

ÏÇ¨Ïö©Î≤ï:
    python scripts/run_pipeline.py
    python scripts/run_pipeline.py --cv-splits 5
    python scripts/run_pipeline.py --skip-package
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
import lightgbm as lgb
import pickle
import subprocess
from src.pipeline import create_pipeline
from src.allocation import smart_allocation
from src.metric import CompetitionMetric
from src.cv_strategy import get_cv_strategy
from src.experiment_tracker import ExperimentTracker
from src.utils import get_logger, load_config, ensure_dir

logger = get_logger(name="pipeline", level="INFO")


def run_full_pipeline(
    cv_splits: int = 5,
    skip_package: bool = False,
    cv_strategy: str = 'purged_walkforward'
):
    """
    Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏ Ïã§Ìñâ.
    
    Parameters
    ----------
    cv_splits : int
        CV split Í∞úÏàò
    skip_package : bool
        Kaggle Ìå®ÌÇ§Ïßï Í±¥ÎÑàÎõ∞Í∏∞
    cv_strategy : str
        CV Ï†ÑÎûµ ('timeseries', 'purged_walkforward')
    """
    config = load_config()
    
    logger.info("=" * 80)
    logger.info("üöÄ FULL PIPELINE EXECUTION (Purged Walk-Forward)")
    logger.info("=" * 80)
    
    # Experiment Name Generation (Early)
    from datetime import datetime
    exp_name = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"üÜî Experiment Name: {exp_name}")
    
    # Create Experiment Directory Early
    tracker = ExperimentTracker(tracking_dir=str(project_root / "experiments"))
    # We will log the full experiment at the end, but we can create the dir now if needed
    # Or just use the name for files in results/
    
    # ========================================
    # Step 1: Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("üìä Step 1: Data Loading & Preprocessing")
    logger.info("=" * 80)
    
    train_path = project_root / config['data']['train']
    logger.info(f"Loading from: {train_path}")
    
    # Ï†ÑÏ≤òÎ¶¨ Pipeline ÏÇ¨Ïö©
    pipeline = create_pipeline(
        # Ï†ÑÏ≤òÎ¶¨
        fillna_strategy=config['features']['fill_missing_strategy'],
        # Feature Engineering
        add_interactions=config['features']['add_interactions'],
        use_time_series_features=config['features']['use_time_series_features'],
        use_advanced_features=config['features']['use_advanced_features'],
        use_market_regime_features=config['features'].get('use_market_regime_features'),
        # Feature Selection (Explicitly handled later)
        # use_feature_selection=False 
    )
    
    df = pd.read_csv(train_path)
    X, y = pipeline.fit_transform(df)
    feature_cols = pipeline.get_feature_names()
    
    # Calculate MA20 for Momentum Guard
    # Assuming 'Close' column exists in raw df
    if 'Close' in df.columns:
        ma20 = df['Close'].rolling(window=20).mean()
        # Trend Signal: True if Close >= MA20 (Bullish), False if Close < MA20 (Bearish)
        # Handle NaN by assuming Bullish (True) to avoid penalty at start
        trend_signal = (df['Close'] >= ma20) | ma20.isna()
    else:
        logger.warning("‚ö†Ô∏è 'Close' column not found. Momentum Guard disabled.")
        trend_signal = pd.Series(True, index=df.index)
        
    logger.info(f"‚úÖ Data loaded: {X.shape[0]} samples, {X.shape[1]} features")
    
    # ÎßàÏßÄÎßâ NÏùº Î∂ÑÎ¶¨ (ÏµúÏ¢Ö ÌÖåÏä§Ìä∏Ïö©) - configÏóêÏÑú ÏùΩÍ∏∞
    holdout_size = config['cv'].get('final_holdout', 180)
    purge_gap = config['cv'].get('purge_gap', 5)
    
    # CV Îç∞Ïù¥ÌÑ∞ÏôÄ ÏµúÏ¢Ö ÌÖåÏä§Ìä∏ ÏÇ¨Ïù¥Ïóê purge_gapÎßåÌÅº Í∞ÑÍ≤©ÏùÑ Îë† (data leakage Î∞©ÏßÄ)
    X_cv = X.iloc[:-(holdout_size + purge_gap)]  # CVÏö© (ÎßàÏßÄÎßâ 180+5=185Í∞ú Ï†úÏô∏)
    y_cv = y.iloc[:-(holdout_size + purge_gap)]
    df_cv = df.iloc[:-(holdout_size + purge_gap)]
    
    X_test = X.iloc[-holdout_size:]  # ÏµúÏ¢Ö ÌÖåÏä§Ìä∏Ïö© (ÎßàÏßÄÎßâ 180Í∞ú)
    y_test = y.iloc[-holdout_size:]
    df_test = df.iloc[-holdout_size:]
    
    logger.info(f"CV data: {len(X_cv)} samples (excluding last {holdout_size + purge_gap} days)")
    logger.info(f"Purge gap: {purge_gap} days between CV and final test")
    logger.info(f"Final test: {len(X_test)} samples (last {holdout_size} days)")
    
    # ========================================
    # Step 2: Î™®Îç∏ ÌïôÏäµ (Purged Walk-Forward CV)
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info(f"ü§ñ Step 2: Model Training ({cv_strategy}, {cv_splits} splits)")
    logger.info("=" * 80)
    
    # Purged Walk-Forward CV
    if cv_strategy == 'purged_walkforward':
        cv = get_cv_strategy(
            'purged_walkforward',
            n_splits=cv_splits,
            train_size=config['cv'].get('train_size', 2000),
            test_size=config['cv'].get('test_size', 500),
            purge_gap=config['cv'].get('purge_gap', 5)
        )
        logger.info("Using Purged Walk-Forward Split:")
        logger.info(f"  ‚Ä¢ Train size: {config['cv'].get('train_size', 2000)}")
        logger.info(f"  ‚Ä¢ Test size: {config['cv'].get('test_size', 500)}")
        logger.info(f"  ‚Ä¢ Purge gap: {config['cv'].get('purge_gap', 5)}")
    else:
        cv = get_cv_strategy('timeseries', n_splits=cv_splits)
        logger.info(f"Using TimeSeriesSplit (n_splits={cv_splits})")
    
    oof_predictions = np.zeros(len(X_cv))
    oof_risk_predictions = np.zeros(len(X_cv)) # Risk OOF
    oof_risk2_predictions = np.zeros(len(X_cv)) # Risk 2 OOF (Market Regime)
    oof_mask = np.zeros(len(X_cv), dtype=bool)
    
    models_return = []
    models_risk = []
    models_risk2 = []
    
    # Custom Objectives & Metrics
    from src.custom_objectives import correlation_metric
    
    # 1. Return Model Params
    return_lgbm_params = config['lgbm_return'].copy()
    
    # 2. Risk Model Params
    risk_lgbm_params = config['lgbm_risk'].copy()

    # 3. Risk Model 2 Params (Market Regime)
    risk2_lgbm_params = config.get('lgbm_risk2', config['lgbm_return'].copy())
    
    # FeatureSelectorÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Risk Target(abs returns)Í≥º ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä ÎÜíÏùÄ Feature ÏÑ†Î≥Ñ
    from src.feature_selection import FeatureSelector
    feature_selector = FeatureSelector()
    
    # 1. Return Model Feature Selection (Importance)
    # We need to select features for Return Model explicitly now
    return_top_k = config['features']['feature_selection']['return_model']['top_k']
    logger.info(f"Selecting Top {return_top_k} features for Return Model...")
    
    return_features = feature_selector.select_by_importance(
        X, y, top_k=return_top_k,
        lgbm_params={'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'seed': 42}
    )
    logger.info(f"Return Model Features: {len(return_features)}")

    # 2. Risk Model Feature Selection (Correlation)
    # Risk Target ÏÉùÏÑ±
    
    # Risk Target ÏÉùÏÑ±
    y_risk_target = y.abs()
    
    # Risk 2 Target (Market Regime)
    # Use market_forward_excess_returns if available, else use forward_returns
    if 'market_forward_excess_returns' in df.columns:
        # Ensure alignment with X (in case pipeline dropped rows)
        y_risk2_target = df.loc[X.index, 'market_forward_excess_returns']
    else:
        logger.warning("‚ö†Ô∏è 'market_forward_excess_returns' not found. Using 'forward_returns' for Risk Model 2.")
        y_risk2_target = y
    
    risk_top_k = config['features']['feature_selection']['risk_model']['top_k']
    risk_features = feature_selector.select_by_correlation(X, y_risk_target, method='spearman', top_k=risk_top_k)
    logger.info(f"Risk Model Features (Top {risk_top_k}): {risk_features[:10]} ...")
    
    # Risk 2 Feature Selection (Correlation + Crash Divergence)
    risk2_top_k = config['features']['feature_selection']['risk2_model']['top_k']
    
    # 1. Correlation
    risk2_corr_features = feature_selector.select_by_correlation(X, y_risk2_target, method='spearman', top_k=risk2_top_k)
    
    # 2. Crash Divergence (Auto-detect crash features)
    # Use default 5% quantile and top 20 features (or read from config if added)
    risk2_crash_features = feature_selector.select_by_crash_divergence(
        X, y_risk2_target, 
        crash_threshold_quantile=config['features']['feature_selection']['risk2_model']['crash_threshold_quantile'], 
        top_k=config['features']['feature_selection']['risk2_model']['top_k'] # Select top 50 divergent features
    )
    
    # Combine and deduplicate
    risk2_features = list(set(risk2_corr_features + risk2_crash_features))
    logger.info(f"Risk Model 2 Features: {len(risk2_features)} (Corr: {len(risk2_corr_features)}, Crash: {len(risk2_crash_features)})")
    logger.info(f"Top Crash Features: {risk2_crash_features[:10]}")
    
    # Metric calculator
    metric_calculator = CompetitionMetric(
        vol_threshold=config['metric']['vol_threshold'],
        use_return_penalty=config['metric']['use_return_penalty'],
        min_periods=config['metric']['min_periods']
    )
    
    fold_scores = []
    
    from src.allocation import risk_adjusted_allocation, triple_model_allocation
    
    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_cv)):
        logger.info(f"\n  Fold {fold_idx + 1}/{cv_splits} | Train: {len(train_idx)} | Val: {len(val_idx)}")
        
        X_train_fold = X_cv.iloc[train_idx]
        y_train_fold = y_cv.iloc[train_idx]
        X_val_fold = X_cv.iloc[val_idx]
        y_val_fold = y_cv.iloc[val_idx]
        
        # 1. Return Model Training
        model_return = lgb.LGBMRegressor(**return_lgbm_params)
        model_return.fit(
            X_train_fold[return_features], y_train_fold,
            eval_set=[(X_val_fold[return_features], y_val_fold)],
            eval_metric=correlation_metric, # Use IC for Early Stopping
            callbacks=[lgb.log_evaluation(0)]
        )
        
        # 2. Risk Model Training (Target: abs(returns))
        y_train_risk = y_train_fold.abs()
        y_val_risk = y_val_fold.abs()
        
        model_risk = lgb.LGBMRegressor(**risk_lgbm_params)
        model_risk.fit(
            X_train_fold[risk_features], y_train_risk,
            eval_set=[(X_val_fold[risk_features], y_val_risk)],
            eval_metric='quantile',
            callbacks=[lgb.log_evaluation(0)]
        )
        
        # 3. Risk Model 2 Training (Target: Market Returns)
        y_train_risk2 = y_risk2_target.iloc[train_idx]
        y_val_risk2 = y_risk2_target.iloc[val_idx]
        
        model_risk2 = lgb.LGBMRegressor(**risk2_lgbm_params)
        model_risk2.fit(
            X_train_fold[risk2_features], y_train_risk2,
            eval_set=[(X_val_fold[risk2_features], y_val_risk2)],
            eval_metric=risk2_lgbm_params['metric'],
            callbacks=[lgb.log_evaluation(0)]
        )
        
        # OOF predictions
        pred_return = model_return.predict(X_val_fold[return_features])
        pred_risk = model_risk.predict(X_val_fold[risk_features])
        pred_risk2 = model_risk2.predict(X_val_fold[risk2_features])
        
        oof_predictions[val_idx] = pred_return
        oof_risk_predictions[val_idx] = pred_risk
        oof_risk2_predictions[val_idx] = pred_risk2
        oof_mask[val_idx] = True
        
        # Calculate Trend Signal (Price >= MA20)
        # We need Close price for validation set.
        # Assuming 'Close' is in df_cv (loaded from train.csv).
        # If 'Close' is not available, try to approximate or skip.
        # Check if 'Close' exists
        if 'Close' in df_cv.columns:
            close_prices = df_cv.iloc[val_idx]['Close']
            # We need rolling mean. But val_idx is a chunk. 
            # We should calculate MA20 for the whole df_cv first.
            pass
        
        # Better approach: Calculate MA20 for entire df at start
        
        # Fold score (Triple Model Allocation)
        # k=1.0, threshold=0.0
        fold_allocations = triple_model_allocation(
            return_pred=pred_return,
            risk_vol_pred=pred_risk,
            risk_market_pred=pred_risk2,
            market_threshold=config['allocation']['threshold'],
            k=config['allocation']['k'],
            trend_signal=trend_signal.iloc[val_idx].values
        )
        
        val_df_fold = df_cv.iloc[val_idx]
        
        fold_result = metric_calculator.calculate_score(
            allocations=fold_allocations,
            forward_returns=val_df_fold['forward_returns'].values,
            market_returns=val_df_fold['forward_returns'].values,
            risk_free_rate=val_df_fold['risk_free_rate'].values
        )
        
        fold_scores.append(fold_result['score'])
        models_return.append(model_return)
        models_risk.append(model_risk)
        models_risk2.append(model_risk2)
        
            # Log metrics (Handle different metric names safely)
        ret_score = model_return.best_score_['valid_0'].get('correlation', 0.0)
        risk_score = model_risk.best_score_['valid_0'].get('quantile', 0.0)
        risk2_score = model_risk2.best_score_['valid_0'].get(risk2_lgbm_params['metric'], 0.0)
        logger.info(f"  ‚úÖ Fold {fold_idx + 1} Score: {fold_result['score']:.6f} | Ret IC: {ret_score:.4f} | Risk Q: {risk_score:.4f} | Mkt {risk2_lgbm_params['metric']}: {risk2_score:.4f}")
    
    logger.info(f"\n‚úÖ CV Training complete | OOF samples: {oof_mask.sum()}/{len(X_cv)}")
    logger.info(f"üìä Fold Scores: {', '.join([f'{s:.4f}' for s in fold_scores])}")
    logger.info(f"üìä Mean ¬± Std: {np.mean(fold_scores):.6f} ¬± {np.std(fold_scores):.6f}")
    
    # ========================================
    # Step 3: OOF ÌèâÍ∞Ä
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("üìà Step 3: OOF Evaluation")
    logger.info("=" * 80)
    
    oof_df = df_cv[oof_mask]
    oof_pred_return = oof_predictions[oof_mask]
    oof_df = df_cv[oof_mask]
    oof_pred_return = oof_predictions[oof_mask]
    oof_pred_risk = oof_risk_predictions[oof_mask]
    oof_pred_risk2 = oof_risk2_predictions[oof_mask]
    
    # Allocation
    # Use Triple Model Allocation
    allocations = triple_model_allocation(
        return_pred=oof_pred_return,
        risk_vol_pred=oof_pred_risk,
        risk_market_pred=oof_pred_risk2,
        market_threshold=config['allocation']['threshold'],
        k=config['allocation']['k'],
        trend_signal=trend_signal.iloc[:len(df_cv)][oof_mask].values
    )
    
    results = metric_calculator.calculate_score(
        allocations=allocations,
        forward_returns=oof_df['forward_returns'].values,
        market_returns=oof_df['forward_returns'].values,
        risk_free_rate=oof_df['risk_free_rate'].values
    )
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üéØ OOF RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"üìä Adjusted Sharpe: {results['score']:.6f}")
    logger.info(f"üìä Sharpe Ratio: {results['sharpe']:.6f}")
    logger.info(f"‚ö†Ô∏è  Vol Penalty: {results['vol_penalty']:.6f}")
    logger.info(f"‚ö†Ô∏è  Return Penalty: {results['return_penalty']:.6f}")
    logger.info(f"üìâ Strategy Vol: {results['strategy_vol']:.2f}%")
    logger.info(f"üìâ Market Vol: {results['market_vol']:.2f}%")
    logger.info(f"{'='*80}")
    
    # Save OOF predictions for analysis
    # Save with Experiment ID
    oof_filename = f'oof_predictions_{exp_name}.csv'
    oof_save_path = project_root / config['output']['submission_dir'] / oof_filename
    
    # Also save as 'latest' for backward compatibility
    oof_latest_path = project_root / config['output']['submission_dir'] / 'oof_predictions.csv'
    oof_df_save = pd.DataFrame({
        'date_id': oof_df['date_id'] if 'date_id' in oof_df.columns else oof_df.index,
        'actual_return': oof_df['forward_returns'].values,
        'pred_return': oof_pred_return,
        'pred_return': oof_pred_return,
        'pred_risk': oof_pred_risk,
        'pred_risk2': oof_pred_risk2,
        'allocation': allocations
    })
    oof_df_save.to_csv(oof_save_path, index=False)
    oof_df_save.to_csv(oof_latest_path, index=False)
    logger.info(f"üíæ OOF predictions saved to: {oof_save_path}")
    
    # ========================================
    # Step 4: ÏµúÏ¢Ö Î™®Îç∏ ÌïôÏäµ (Ï†ÑÏ≤¥ CV Îç∞Ïù¥ÌÑ∞)
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("üèÅ Step 4: Final Model Training (CV data only)")
    logger.info("=" * 80)
    
    # Return Model
    final_model_return = lgb.LGBMRegressor(**return_lgbm_params)
    final_model_return.fit(X_cv[return_features], y_cv, eval_metric=correlation_metric)
    
    # Risk Model
    final_model_risk = lgb.LGBMRegressor(**risk_lgbm_params)
    final_model_risk.fit(X_cv[risk_features], y_cv.abs(), eval_metric='quantile')
    
    # Risk Model 2
    final_model_risk2 = lgb.LGBMRegressor(**risk2_lgbm_params)
    final_model_risk2.fit(X_cv[risk2_features], y_risk2_target.iloc[:len(X_cv)], eval_metric=risk2_lgbm_params['metric'])
    
    logger.info(f"‚úÖ Final models trained on {len(X_cv)} samples")
    
    # ========================================
    # Step 5: ÏµúÏ¢Ö ÌÖåÏä§Ìä∏ (ÎßàÏßÄÎßâ 180Ïùº)
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("üß™ Step 5: Final Test Evaluation (Last 180 days)")
    logger.info("=" * 80)
    
    test_pred_ret = final_model_return.predict(X_test[return_features])
    test_pred_ret = final_model_return.predict(X_test[return_features])
    test_pred_risk = final_model_risk.predict(X_test[risk_features])
    test_pred_risk2 = final_model_risk2.predict(X_test[risk2_features])
    
    # Triple Allocation for Test
    test_allocations = triple_model_allocation(
        return_pred=test_pred_ret,
        risk_vol_pred=test_pred_risk,
        risk_market_pred=test_pred_risk2,
        market_threshold=config['allocation']['threshold'],
        k=config['allocation']['k'],
        trend_signal=trend_signal.iloc[-holdout_size:].values
    )
    
    test_results = metric_calculator.calculate_score(
        allocations=test_allocations,
        forward_returns=df_test['forward_returns'].values,
        market_returns=df_test['forward_returns'].values,
        risk_free_rate=df_test['risk_free_rate'].values
    )
    
    # Save Test predictions for analysis
    test_filename = f'test_predictions_{exp_name}.csv'
    test_save_path = project_root / config['output']['submission_dir'] / test_filename
    test_latest_path = project_root / config['output']['submission_dir'] / 'test_predictions.csv'
    test_df_save = pd.DataFrame({
        'date_id': df_test['date_id'] if 'date_id' in df_test.columns else df_test.index,
        'actual_return': df_test['forward_returns'].values,
        'pred_return': test_pred_ret,
        'pred_return': test_pred_ret,
        'pred_risk': test_pred_risk,
        'pred_risk2': test_pred_risk2,
        'allocation': test_allocations,
    })
    test_df_save.to_csv(test_save_path, index=False)
    test_df_save.to_csv(test_latest_path, index=False)
    logger.info(f"üíæ Test predictions saved to: {test_save_path}")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üéØ FINAL TEST RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"üìä Adjusted Sharpe: {test_results['score']:.6f}")
    logger.info(f"üìä Sharpe Ratio: {test_results['sharpe']:.6f}")
    logger.info(f"‚ö†Ô∏è  Vol Penalty: {test_results['vol_penalty']:.6f}")
    logger.info(f"‚ö†Ô∏è  Return Penalty: {test_results['return_penalty']:.6f}")
    logger.info(f"{'='*80}")
    
    logger.info(f"\nüí° OOF vs Final Test:")
    logger.info(f"   OOF:  {results['score']:.6f}")
    logger.info(f"   Test: {test_results['score']:.6f}")
    logger.info(f"   Diff: {(test_results['score'] - results['score']):.6f}")
    
    # ÏµúÏ¢Ö Î™®Îç∏ Ïû¨ÌïôÏäµ (Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞)
    logger.info(f"\nüîÑ Retraining on ALL data for submission...")
    
    final_model_return_all = lgb.LGBMRegressor(**return_lgbm_params)
    final_model_return_all.fit(X[return_features], y)
    
    final_model_risk_all = lgb.LGBMRegressor(**risk_lgbm_params)
    final_model_risk_all = lgb.LGBMRegressor(**risk_lgbm_params)
    final_model_risk_all.fit(X[risk_features], y.abs())
    
    final_model_risk2_all = lgb.LGBMRegressor(**risk2_lgbm_params)
    final_model_risk2_all.fit(X[risk2_features], y_risk2_target)
    
    logger.info(f"‚úÖ Final models retrained on {len(X)} samples")
    
    # ========================================
    # Step 6: Î™®Îç∏ Ï†ÄÏû•
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("üíæ Step 6: Saving Model")
    logger.info("=" * 80)
    
    model_dir = project_root / config['output']['model_dir']
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # Save Model (Consolidated)
    model_save_path = model_dir / 'triple_model.pkl'
    with open(model_save_path, 'wb') as f:
        pickle.dump({
            'model_return': final_model_return_all,
            'model_risk': final_model_risk_all,
            'model_risk2': final_model_risk2_all,
            'pipeline': pipeline,
            'feature_cols': feature_cols, # All generated features
            'return_features': return_features,
            'risk_features': risk_features,
            'risk2_features': risk2_features,
            'config': config,
            'oof_score': results['score'],
            'test_score': test_results['score']
        }, f)
        
    logger.info(f"‚úÖ Model saved: {model_save_path}")
    
    # Feature importance (Return Model)
    importance_df = pd.DataFrame({
        'feature': return_features,
        'importance': final_model_return_all.feature_importances_
    }).sort_values('importance', ascending=False)
    
    importance_path = project_root / config['output']['submission_dir'] / 'feature_importance_return.csv'
    importance_df.to_csv(importance_path, index=False)
    
    logger.info(f"\nTop 5 features (Return Model):")
    for idx, row in importance_df.head(5).iterrows():
        logger.info(f"  {row['feature']}: {row['importance']:.2f}")
    
    # ========================================
    # Step 7: Kaggle Ìå®ÌÇ§Ïßï
    # ========================================
    if skip_package:
        logger.info("\n" + "=" * 80)
        logger.info("üì¶ Step 7: Kaggle Packaging")
        logger.info("=" * 80)
        
        package_script = project_root / "scripts" / "package_for_kaggle.sh"
        try:
            result = subprocess.run(
                ["bash", str(package_script)],
                cwd=project_root,
                capture_output=True,
                text=True
            )
            logger.info(result.stdout)
            if result.returncode == 0:
                logger.info("‚úÖ Kaggle package created: kaggle_submission.zip")
            else:
                logger.warning(f"‚ö†Ô∏è  Packaging failed: {result.stderr}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Packaging error: {e}")
    
    # ========================================
    # Final Summary & Experiment Tracking
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("üéâ PIPELINE COMPLETE!")
    logger.info("=" * 80)
    
    # Ïã§Ìóò Ï∂îÏ†Å
    # tracker is already initialized
    
    # Ïã§Ìóò Ïù¥Î¶Ñ ÏÉùÏÑ± (Already done)
    # from datetime import datetime
    # exp_name = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # Ïã§Ìóò Ï†ïÎ≥¥ Ï†ÄÏû•
    exp_config = {
        "cv_strategy": cv_strategy,
        "cv_splits": cv_splits,
        "train_size": 2000 if cv_strategy == 'purged_walkforward' else len(X_cv),
        "test_size": 500 if cv_strategy == 'purged_walkforward' else 0,
        "return_n_estimators": return_lgbm_params.get('n_estimators'),
        "return_learning_rate": return_lgbm_params.get('learning_rate'),
        "risk_n_estimators": risk_lgbm_params.get('n_estimators'),
        "risk_learning_rate": risk_lgbm_params.get('learning_rate'),
        "k": config['allocation']['k'],
        "threshold": config['allocation']['threshold'],
        "return_top_k": config['features']['feature_selection']['return_model']['top_k'],
        "risk_top_k": config['features']['feature_selection']['risk_model']['top_k'],
        "risk2_top_k": config['features']['feature_selection']['risk2_model']['top_k'],
    }
    
    exp_results = {
        "oof_score": results['score'],
        "oof_sharpe": results['sharpe'],
        "oof_vol_penalty": results['vol_penalty'],
        "oof_return_penalty": results['return_penalty'],
        "test_score": test_results['score'],
        "test_sharpe": test_results['sharpe'],
        "test_vol_penalty": test_results['vol_penalty'],
        "test_return_penalty": test_results['return_penalty'],
        "cv_mean": np.mean(fold_scores),
        "cv_std": np.std(fold_scores),
        "n_features": len(feature_cols),
        # Model Performance Metrics (Approximate from last fold or mean if tracked)
        # We didn't track mean IC/Quantile across folds in variables, but we can add them.
        # For now, let's just log the OOF score which is the main one.
        # But user asked for "individual model performance".
        # Let's calculate them on OOF predictions.
        "return_ic": np.corrcoef(oof_pred_return, oof_df['forward_returns'])[0, 1],
        "risk_quantile": np.mean(np.maximum(0.75 * (oof_df['forward_returns'].abs() - oof_pred_risk), (0.75 - 1) * (oof_df['forward_returns'].abs() - oof_pred_risk))), # Negative of score
        # Risk 2 RMSE (Market Regime)
        # We need y_risk2_target aligned with OOF.
        # It's tricky because y_risk2_target might be different from forward_returns.
        # But we have oof_pred_risk2.
        # Let's skip complex calc here and rely on CV logs or add properly later.
    }
    
    exp_notes = f"Purged WF baseline - Top feature: {importance_df.iloc[0]['feature']}"
    
    exp_dir = tracker.log_experiment(
        name=exp_name,
        config=exp_config,
        results=exp_results,
        notes=exp_notes
    )
    
    logger.info(f"\nüíæ Experiment logged: {exp_name}")
    logger.info(f"   Directory: {exp_dir}")
    
    # Save config files for reproducibility
    import shutil
    for filename in ['config.yaml', 'best_params.yaml']:
        src_path = project_root / 'conf' / filename
        if src_path.exists():
            dst_path = Path(exp_dir) / filename
            shutil.copy(src_path, dst_path)
            logger.info(f"   Saved {filename} to experiment dir")
    
    logger.info(f"\nüìä Summary:")
    logger.info(f"  ‚Ä¢ Experiment: {exp_name}")
    logger.info(f"  ‚Ä¢ CV Strategy: {cv_strategy}")
    logger.info(f"  ‚Ä¢ CV Folds: {cv_splits}")
    logger.info(f"  ‚Ä¢ OOF Score: {results['score']:.6f}")
    logger.info(f"  ‚Ä¢ Test Score: {test_results['score']:.6f} (Last 180 days)")
    logger.info(f"  ‚Ä¢ Fold Scores: {', '.join([f'{s:.4f}' for s in fold_scores])}")
    logger.info(f"  ‚Ä¢ CV Mean ¬± Std: {np.mean(fold_scores):.6f} ¬± {np.std(fold_scores):.6f}")
    logger.info(f"  ‚Ä¢ Model: {model_dir}")
    logger.info(f"  ‚Ä¢ Features: {len(feature_cols)}")
    logger.info(f"\nüìå Next Steps:")
    logger.info(f"  1. Review Test score: {test_results['score']:.6f}")
    logger.info(f"  2. View experiments: experiments/experiments.csv")
    logger.info(f"  3. Upload kaggle_submission.zip to Kaggle Dataset")
    logger.info(f"  4. Submit using kaggle_submission_universal.ipynb")
    logger.info(f"  5. Compare with expected Public LB: ~{test_results['score']:.3f}")
    logger.info("=" * 80)


def main():
    import argparse
    config = load_config()
    # parser = argparse.ArgumentParser(description="Run full training pipeline")
    # parser.add_argument('--cv-splits', type=int, default=config['cv']['n_splits'], help='Number of CV splits')
    # parser.add_argument('--skip-package', action='store_true', default=config['output']['skip_package'], help='Skip Kaggle packaging')
    # parser.add_argument('--cv-strategy', type=str, default=config['cv']['strategy'],
    #                    choices=['timeseries', 'purged_walkforward'],
    #                    help='CV strategy to use')
    # args = parser.parse_args()
    
    run_full_pipeline(
        cv_splits=config['cv']['n_splits'],
        skip_package=config['output']['skip_package'],
        cv_strategy=config['cv']['strategy']
    )


if __name__ == "__main__":
    main()
