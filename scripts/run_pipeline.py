"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (Purged Walk-Forward CV)

í•œ ë²ˆì˜ ì‹¤í–‰ìœ¼ë¡œ:
1. ë°ì´í„° ì „ì²˜ë¦¬
2. ëª¨ë¸ í•™ìŠµ (Purged Walk-Forward CV)
3. OOF í‰ê°€
4. ìµœì¢… í…ŒìŠ¤íŠ¸ (ë§ˆì§€ë§‰ 180ì¼)
5. Kaggle íŒ¨í‚¤ì§•

ì‚¬ìš©ë²•:
    python scripts/run_pipeline.py
    python scripts/run_pipeline.py --cv-splits 5
    python scripts/run_pipeline.py --skip-package
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np
import lightgbm as lgb
import pickle
import subprocess
from src.pipeline import create_pipeline
from src.allocation import smart_allocation
from src.metric import CompetitionMetric
from src.cv_strategy import get_cv_strategy
from src.experiment_tracker import ExperimentTracker
from src.utils import get_logger, load_config, ensure_dir

logger = get_logger(name="pipeline", level="INFO")


def run_full_pipeline(
    cv_splits: int = 5,
    skip_package: bool = False,
    cv_strategy: str = 'purged_walkforward'
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.
    
    Parameters
    ----------
    cv_splits : int
        CV split ê°œìˆ˜
    skip_package : bool
        Kaggle íŒ¨í‚¤ì§• ê±´ë„ˆë›°ê¸°
    cv_strategy : str
        CV ì „ëµ ('timeseries', 'purged_walkforward')
    """
    config = load_config()
    
    logger.info("=" * 80)
    logger.info("ğŸš€ FULL PIPELINE EXECUTION (Purged Walk-Forward)")
    logger.info("=" * 80)
    
    # ========================================
    # Step 1: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š Step 1: Data Loading & Preprocessing")
    logger.info("=" * 80)
    
    train_path = project_root / config['data']['train']
    logger.info(f"Loading from: {train_path}")
    
    # ì „ì²˜ë¦¬ Pipeline ì‚¬ìš©
    pipeline = create_pipeline(
        # ì „ì²˜ë¦¬
        fillna_strategy=config['features']['fill_missing_strategy'],
        # Feature Engineering
        add_interactions=config['features']['add_interactions'],
        use_time_series_features=config['features']['use_time_series_features'],
        use_advanced_features=config['features']['use_advanced_features'],
        use_market_regime_features=config['features'].get('use_market_regime_features'),
        # Feature Selection
        use_feature_selection=config['features']['feature_selection']['enabled'],
        feature_selection_method=config['features']['feature_selection']['method'],
        top_k_features=config['features']['feature_selection']['top_k']
    )
    
    df = pd.read_csv(train_path)
    X, y = pipeline.fit_transform(df)
    feature_cols = pipeline.get_feature_names()
    
    logger.info(f"âœ… Data loaded: {X.shape[0]} samples, {X.shape[1]} features")
    
    # ë§ˆì§€ë§‰ Nì¼ ë¶„ë¦¬ (ìµœì¢… í…ŒìŠ¤íŠ¸ìš©) - configì—ì„œ ì½ê¸°
    holdout_size = config['cv'].get('final_holdout', 180)
    purge_gap = config['cv'].get('purge_gap', 5)
    
    # CV ë°ì´í„°ì™€ ìµœì¢… í…ŒìŠ¤íŠ¸ ì‚¬ì´ì— purge_gapë§Œí¼ ê°„ê²©ì„ ë‘  (data leakage ë°©ì§€)
    X_cv = X.iloc[:-(holdout_size + purge_gap)]  # CVìš© (ë§ˆì§€ë§‰ 180+5=185ê°œ ì œì™¸)
    y_cv = y.iloc[:-(holdout_size + purge_gap)]
    df_cv = df.iloc[:-(holdout_size + purge_gap)]
    
    X_test = X.iloc[-holdout_size:]  # ìµœì¢… í…ŒìŠ¤íŠ¸ìš© (ë§ˆì§€ë§‰ 180ê°œ)
    y_test = y.iloc[-holdout_size:]
    df_test = df.iloc[-holdout_size:]
    
    logger.info(f"CV data: {len(X_cv)} samples (excluding last {holdout_size + purge_gap} days)")
    logger.info(f"Purge gap: {purge_gap} days between CV and final test")
    logger.info(f"Final test: {len(X_test)} samples (last {holdout_size} days)")
    
    # ========================================
    # Step 2: ëª¨ë¸ í•™ìŠµ (Purged Walk-Forward CV)
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info(f"ğŸ¤– Step 2: Model Training ({cv_strategy}, {cv_splits} splits)")
    logger.info("=" * 80)
    
    # Purged Walk-Forward CV
    if cv_strategy == 'purged_walkforward':
        cv = get_cv_strategy(
            'purged_walkforward',
            n_splits=cv_splits,
            train_size=config['cv'].get('train_size', 2000),
            test_size=config['cv'].get('test_size', 500),
            purge_gap=config['cv'].get('purge_gap', 5)
        )
        logger.info("Using Purged Walk-Forward Split:")
        logger.info(f"  â€¢ Train size: {config['cv'].get('train_size', 2000)}")
        logger.info(f"  â€¢ Test size: {config['cv'].get('test_size', 500)}")
        logger.info(f"  â€¢ Purge gap: {config['cv'].get('purge_gap', 5)}")
    else:
        cv = get_cv_strategy('timeseries', n_splits=cv_splits)
        logger.info(f"Using TimeSeriesSplit (n_splits={cv_splits})")
    
    oof_predictions = np.zeros(len(X_cv))
    oof_mask = np.zeros(len(X_cv), dtype=bool)
    models = []
    
    lgbm_params = config['lgbm'].copy()
    
    # Metric calculator
    metric_calculator = CompetitionMetric(
        vol_threshold=config['metric']['vol_threshold'],
        use_return_penalty=config['metric']['use_return_penalty'],
        min_periods=config['metric']['min_periods']
    )
    
    fold_scores = []
    
    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_cv)):
        logger.info(f"\n  Fold {fold_idx + 1}/{cv_splits} | Train: {len(train_idx)} | Val: {len(val_idx)}")
        
        X_train_fold = X_cv.iloc[train_idx]
        y_train_fold = y_cv.iloc[train_idx]
        X_val_fold = X_cv.iloc[val_idx]
        y_val_fold = y_cv.iloc[val_idx]
        
        model = lgb.LGBMRegressor(**lgbm_params)
        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], eval_metric='rmse')
        
        # OOF predictions
        fold_pred = model.predict(X_val_fold)
        oof_predictions[val_idx] = fold_pred
        oof_mask[val_idx] = True
        
        # Fold score
        fold_allocations = smart_allocation(fold_pred, center=1.0, sensitivity=20)
        val_df_fold = df_cv.iloc[val_idx]
        
        fold_result = metric_calculator.calculate_score(
            allocations=fold_allocations,
            forward_returns=val_df_fold['forward_returns'].values,
            market_returns=val_df_fold['forward_returns'].values,
            risk_free_rate=val_df_fold['risk_free_rate'].values
        )
        
        fold_scores.append(fold_result['score'])
        models.append(model)
        
        logger.info(f"  âœ… Fold {fold_idx + 1} Train_idx: {train_idx[0]} ~ {train_idx[-1]} | Val_idx: {val_idx[0]} ~ {val_idx[-1]} | Score: {fold_result['score']:.6f}")
    
    logger.info(f"\nâœ… CV Training complete | OOF samples: {oof_mask.sum()}/{len(X_cv)}")
    logger.info(f"ğŸ“Š Fold Scores: {', '.join([f'{s:.4f}' for s in fold_scores])}")
    logger.info(f"ğŸ“Š Mean Â± Std: {np.mean(fold_scores):.6f} Â± {np.std(fold_scores):.6f}")
    
    # ========================================
    # Step 3: OOF í‰ê°€
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ˆ Step 3: OOF Evaluation")
    logger.info("=" * 80)
    
    oof_df = df_cv[oof_mask]
    oof_pred = oof_predictions[oof_mask]
    
    # Allocation
    allocations = smart_allocation(oof_pred, center=1.0, sensitivity=20)
    
    results = metric_calculator.calculate_score(
        allocations=allocations,
        forward_returns=oof_df['forward_returns'].values,
        market_returns=oof_df['forward_returns'].values,
        risk_free_rate=oof_df['risk_free_rate'].values
    )
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ¯ OOF RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"ğŸ“Š Adjusted Sharpe: {results['score']:.6f}")
    logger.info(f"ğŸ“Š Sharpe Ratio: {results['sharpe']:.6f}")
    logger.info(f"âš ï¸  Vol Penalty: {results['vol_penalty']:.6f}")
    logger.info(f"âš ï¸  Return Penalty: {results['return_penalty']:.6f}")
    logger.info(f"ğŸ“‰ Strategy Vol: {results['strategy_vol']:.2f}%")
    logger.info(f"ğŸ“‰ Market Vol: {results['market_vol']:.2f}%")
    logger.info(f"{'='*80}")
    
    # ========================================
    # Step 4: ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì „ì²´ CV ë°ì´í„°)
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ Step 4: Final Model Training (CV data only)")
    logger.info("=" * 80)
    
    final_model = lgb.LGBMRegressor(**lgbm_params)
    final_model.fit(X_cv, y_cv)
    
    logger.info(f"âœ… Final model trained on {len(X_cv)} samples")
    
    # ========================================
    # Step 5: ìµœì¢… í…ŒìŠ¤íŠ¸ (ë§ˆì§€ë§‰ 180ì¼)
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ§ª Step 5: Final Test Evaluation (Last 180 days)")
    logger.info("=" * 80)
    
    test_pred = final_model.predict(X_test)
    test_allocations = smart_allocation(test_pred, center=1.0, sensitivity=12)
    
    test_results = metric_calculator.calculate_score(
        allocations=test_allocations,
        forward_returns=df_test['forward_returns'].values,
        market_returns=df_test['forward_returns'].values,
        risk_free_rate=df_test['risk_free_rate'].values
    )
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ¯ FINAL TEST RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"ğŸ“Š Adjusted Sharpe: {test_results['score']:.6f}")
    logger.info(f"ğŸ“Š Sharpe Ratio: {test_results['sharpe']:.6f}")
    logger.info(f"âš ï¸  Vol Penalty: {test_results['vol_penalty']:.6f}")
    logger.info(f"âš ï¸  Return Penalty: {test_results['return_penalty']:.6f}")
    logger.info(f"{'='*80}")
    
    logger.info(f"\nğŸ’¡ OOF vs Final Test:")
    logger.info(f"   OOF:  {results['score']:.6f}")
    logger.info(f"   Test: {test_results['score']:.6f}")
    logger.info(f"   Diff: {(test_results['score'] - results['score']):.6f}")
    
    # ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ (ì „ì²´ ë°ì´í„°)
    logger.info(f"\nğŸ”„ Retraining on ALL data for submission...")
    final_model_all = lgb.LGBMRegressor(**lgbm_params)
    final_model_all.fit(X, y)
    logger.info(f"âœ… Final model retrained on {len(X)} samples")
    
    # ========================================
    # Step 6: ëª¨ë¸ ì €ì¥
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ’¾ Step 6: Saving Model")
    logger.info("=" * 80)
    
    model_dir = project_root / config['output']['model_dir']
    ensure_dir(model_dir)
    
    model_path = model_dir / "simple_model.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump({
            'model': final_model_all,  # ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸
            'feature_cols': feature_cols,
            'config': config,
            'oof_score': results['score'],
            'test_score': test_results['score'],
            'cv_models': models,
            'pipeline': pipeline
        }, f)
    
    logger.info(f"âœ… Model saved: {model_path}")
    
    # Feature importance
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': final_model_all.feature_importances_
    }).sort_values('importance', ascending=False)
    
    importance_path = project_root / config['output']['submission_dir'] / 'feature_importance.csv'
    importance_df.to_csv(importance_path, index=False)
    
    logger.info(f"\nTop 5 features:")
    for idx, row in importance_df.head(5).iterrows():
        logger.info(f"  {row['feature']}: {row['importance']:.2f}")
    
    # ========================================
    # Step 7: Kaggle íŒ¨í‚¤ì§•
    # ========================================
    if skip_package:
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“¦ Step 7: Kaggle Packaging")
        logger.info("=" * 80)
        
        package_script = project_root / "scripts" / "package_for_kaggle.sh"
        try:
            result = subprocess.run(
                ["bash", str(package_script)],
                cwd=project_root,
                capture_output=True,
                text=True
            )
            logger.info(result.stdout)
            if result.returncode == 0:
                logger.info("âœ… Kaggle package created: kaggle_submission.zip")
            else:
                logger.warning(f"âš ï¸  Packaging failed: {result.stderr}")
        except Exception as e:
            logger.warning(f"âš ï¸  Packaging error: {e}")
    
    # ========================================
    # Final Summary & Experiment Tracking
    # ========================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ PIPELINE COMPLETE!")
    logger.info("=" * 80)
    
    # ì‹¤í—˜ ì¶”ì 
    tracker = ExperimentTracker(tracking_dir=str(project_root / "experiments"))
    
    # ì‹¤í—˜ ì´ë¦„ ìƒì„±
    from datetime import datetime
    exp_name = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # ì‹¤í—˜ ì •ë³´ ì €ì¥
    exp_config = {
        "cv_strategy": cv_strategy,
        "cv_splits": cv_splits,
        "train_size": 2000 if cv_strategy == 'purged_walkforward' else len(X_cv),
        "test_size": 500 if cv_strategy == 'purged_walkforward' else 0,
        "lgbm_n_estimators": lgbm_params.get('n_estimators'),
        "lgbm_learning_rate": lgbm_params.get('learning_rate'),
    }
    
    exp_results = {
        "oof_score": results['score'],
        "oof_sharpe": results['sharpe'],
        "test_score": test_results['score'],
        "test_sharpe": test_results['sharpe'],
        "cv_mean": np.mean(fold_scores),
        "cv_std": np.std(fold_scores),
        "n_features": len(feature_cols)
    }
    
    exp_notes = f"Purged WF baseline - Top feature: {importance_df.iloc[0]['feature']}"
    
    exp_dir = tracker.log_experiment(
        name=exp_name,
        config=exp_config,
        results=exp_results,
        notes=exp_notes
    )
    
    logger.info(f"\nğŸ’¾ Experiment logged: {exp_name}")
    logger.info(f"   Directory: {exp_dir}")
    
    logger.info(f"\nğŸ“Š Summary:")
    logger.info(f"  â€¢ Experiment: {exp_name}")
    logger.info(f"  â€¢ CV Strategy: {cv_strategy}")
    logger.info(f"  â€¢ CV Folds: {cv_splits}")
    logger.info(f"  â€¢ OOF Score: {results['score']:.6f}")
    logger.info(f"  â€¢ Test Score: {test_results['score']:.6f} (Last 180 days)")
    logger.info(f"  â€¢ Fold Scores: {', '.join([f'{s:.4f}' for s in fold_scores])}")
    logger.info(f"  â€¢ CV Mean Â± Std: {np.mean(fold_scores):.6f} Â± {np.std(fold_scores):.6f}")
    logger.info(f"  â€¢ Model: {model_path}")
    logger.info(f"  â€¢ Features: {len(feature_cols)}")
    logger.info(f"\nğŸ“Œ Next Steps:")
    logger.info(f"  1. Review Test score: {test_results['score']:.6f}")
    logger.info(f"  2. View experiments: experiments/experiments.csv")
    logger.info(f"  3. Upload kaggle_submission.zip to Kaggle Dataset")
    logger.info(f"  4. Submit using kaggle_submission_universal.ipynb")
    logger.info(f"  5. Compare with expected Public LB: ~{test_results['score']:.3f}")
    logger.info("=" * 80)


def main():
    import argparse
    config = load_config()
    # parser = argparse.ArgumentParser(description="Run full training pipeline")
    # parser.add_argument('--cv-splits', type=int, default=config['cv']['n_splits'], help='Number of CV splits')
    # parser.add_argument('--skip-package', action='store_true', default=config['output']['skip_package'], help='Skip Kaggle packaging')
    # parser.add_argument('--cv-strategy', type=str, default=config['cv']['strategy'],
    #                    choices=['timeseries', 'purged_walkforward'],
    #                    help='CV strategy to use')
    # args = parser.parse_args()
    
    run_full_pipeline(
        cv_splits=config['cv']['n_splits'],
        skip_package=config['output']['skip_package'],
        cv_strategy=config['cv']['strategy']
    )


if __name__ == "__main__":
    main()
