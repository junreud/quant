"""
Feature Selection Module

í”¼ì²˜ ì„ íƒì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤.
"""

import pandas as pd
import numpy as np
from typing import List, Union
import lightgbm as lgb
from src.utils import get_logger

logger = get_logger(name="feature_selection", level="INFO")

class FeatureSelector:
    """
    í”¼ì²˜ ì„ íƒ í´ë˜ìŠ¤.
    
    ê¸°ëŠ¥:
    1. ìƒê´€ê´€ê³„ ê¸°ë°˜ ë‹¤ì¤‘ê³µì„ ì„± ì œê±°
    2. LGBM Feature Importance ê¸°ë°˜ ì„ íƒ
    """
    
    def __init__(self):
        self.selected_features = None
        
    def remove_collinear(self, df: pd.DataFrame, threshold: float = 0.95) -> List[str]:
        """
        ìƒê´€ê´€ê³„ê°€ ë†’ì€ í”¼ì²˜ ì œê±°.
        
        Parameters
        ----------
        df : pd.DataFrame
            í”¼ì²˜ ë°ì´í„°
        threshold : float
            ìƒê´€ê´€ê³„ ì„ê³„ê°’ (ì ˆëŒ€ê°’ ê¸°ì¤€)
            
        Returns
        -------
        List[str]
            ì„ íƒëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” Removing collinear features (threshold={threshold})...")
        
        # ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚°
        corr_matrix = df.corr().abs()
        
        # ìƒì‚¼ê°í–‰ë ¬ë§Œ ì„ íƒ
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        
        # ì„ê³„ê°’ ë„˜ëŠ” ì»¬ëŸ¼ ì°¾ê¸°
        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]
        
        selected = [col for col in df.columns if col not in to_drop]
        
        logger.info(f"   Dropped {len(to_drop)} features. Remaining: {len(selected)}")
        return selected

    def select_by_importance(self, X: pd.DataFrame, y: pd.Series, top_k: int = 50, 
                           lgbm_params: dict = None) -> List[str]:
        """
        LGBM Feature Importance ê¸°ë°˜ í”¼ì²˜ ì„ íƒ.
        
        Parameters
        ----------
        X : pd.DataFrame
            í”¼ì²˜ ë°ì´í„°
        y : pd.Series
            íƒ€ê²Ÿ ë°ì´í„°
        top_k : int
            ì„ íƒí•  ìƒìœ„ í”¼ì²˜ ê°œìˆ˜
        lgbm_params : dict
            LGBM íŒŒë¼ë¯¸í„° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns
        -------
        List[str]
            ì„ íƒëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” Selecting top {top_k} features by LGBM importance...")
        
        if lgbm_params is None:
            lgbm_params = {
                'objective': 'regression',
                'metric': 'rmse',
                'verbosity': -1,
                'seed': 42
            }
            
        # LGBM Dataset
        dtrain = lgb.Dataset(X, label=y)
        
        # Train model (lightweight)
        model = lgb.train(lgbm_params, dtrain, num_boost_round=100)
        
        # Get importance
        importance = pd.DataFrame({
            'feature': X.columns,
            'importance': model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)
        
        # Select top k
        selected = importance.head(top_k)['feature'].tolist()
        
        logger.info(f"   Selected {len(selected)} features.")
        return selected

    def select_by_correlation(self, X: pd.DataFrame, y: pd.Series, method: str = 'spearman', top_k: int = 20) -> List[str]:
        """
        ìƒê´€ê´€ê³„ ê¸°ë°˜ í”¼ì²˜ ì„ íƒ.
        
        Parameters
        ----------
        X : pd.DataFrame
            í”¼ì²˜ ë°ì´í„°
        y : pd.Series
            íƒ€ê²Ÿ ë°ì´í„°
        method : str
            ìƒê´€ê³„ìˆ˜ ë°©ë²• ('pearson', 'spearman')
        top_k : int
            ì„ íƒí•  ìƒìœ„ í”¼ì²˜ ê°œìˆ˜
            
        Returns
        -------
        List[str]
            ì„ íƒëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” Selecting top {top_k} features by {method} correlation...")
        
        # ë°ì´í„° ë³‘í•© (ì¸ë±ìŠ¤ ê¸°ì¤€)
        # Xì™€ yì˜ ì¸ë±ìŠ¤ê°€ ë§ì•„ì•¼ í•¨
        
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        corrs = X.corrwith(y, method=method).abs()
        
        # ìƒìœ„ kê°œ ì„ ì •
        selected = corrs.sort_values(ascending=False).head(top_k).index.tolist()
        
        logger.info(f"   Selected {len(selected)} features.")
        return selected
    def select_by_crash_divergence(self, X: pd.DataFrame, y: pd.Series, 
                                 crash_threshold_quantile: float = 0.05, 
                                 top_k: int = 20) -> List[str]:
        """
        ì‹œì¥ í­ë½(Crash) ì‹œì ê³¼ í‰ìƒì‹œì˜ Feature ë¶„í¬ ì°¨ì´(Divergence)ê°€ í° Feature ì„ íƒ.
        
        Parameters
        ----------
        X : pd.DataFrame
            í”¼ì²˜ ë°ì´í„°
        y : pd.Series
            íƒ€ê²Ÿ ë°ì´í„° (Market Returns)
        crash_threshold_quantile : float
            Crashë¡œ ì •ì˜í•  í•˜ìœ„ ë¶„ìœ„ìˆ˜ (ì˜ˆ: 0.05 = í•˜ìœ„ 5%)
        top_k : int
            ì„ íƒí•  ìƒìœ„ í”¼ì²˜ ê°œìˆ˜
            
        Returns
        -------
        List[str]
            ì„ íƒëœ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ” Selecting top {top_k} features by Crash Divergence (q={crash_threshold_quantile})...")
        
        # Align indices
        common_idx = X.index.intersection(y.index)
        X_aligned = X.loc[common_idx]
        y_aligned = y.loc[common_idx]
        
        # Define Crash Mask
        crash_threshold = y_aligned.quantile(crash_threshold_quantile)
        crash_mask = y_aligned < crash_threshold
        
        n_crash = crash_mask.sum()
        logger.info(f"   Identified {n_crash} crash periods (Threshold: {crash_threshold:.4f})")
        
        if n_crash < 10:
            logger.warning("   Too few crash periods for reliable analysis. Returning empty list.")
            return []
            
        # Calculate Means
        crash_means = X_aligned[crash_mask].mean()
        normal_means = X_aligned[~crash_mask].mean()
        
        # Calculate Divergence (Z-score like difference)
        # (Crash Mean - Normal Mean) / Overall Std
        # Avoid division by zero
        overall_std = X_aligned.std() + 1e-8
        divergence = (crash_means - normal_means) / overall_std
        
        # Select top k by absolute divergence
        selected = divergence.abs().sort_values(ascending=False).head(top_k).index.tolist()
        
        logger.info(f"   Selected {len(selected)} features.")
        return selected
